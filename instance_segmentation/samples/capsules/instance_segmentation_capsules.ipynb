{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyrealsense2 as rs\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "import scipy\n",
    "import random\n",
    "import math\n",
    "import skimage.io\n",
    "import datetime\n",
    "import open3d as o3d\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../../\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "\n",
    "Capsules_DIR = os.path.join(ROOT_DIR, \"datasets/capsules\")\n",
    "\n",
    "\n",
    "import capsules\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "CAPSULES_MODEL_PATH = \"../../mask_rcnn_capsule_0100.h5\"\n",
    "\n",
    "# Directory of images to run detection on\n",
    "IMAGE_DIR = os.path.join(ROOT_DIR, \"images\")\n",
    "\n",
    "from poke_grasp.msg import stone_pose\n",
    "#from dig-grasping.msg import stone_pose\n",
    "import rospy\n",
    "import geometry_msgs.msg\n",
    "import time\n",
    "import actionlib\n",
    "from std_msgs.msg import String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rospy.init_node('capsule_segmentation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_directories():\n",
    "    if not os.path.exists(\"JPEGImages/\"):\n",
    "        os.makedirs(\"JPEGImages/\")\n",
    "    if not os.path.exists(\"depth/\"):\n",
    "        os.makedirs(\"depth/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start instance segmentation by Mask RCNN\n",
    "class InferenceConfig(capsules.CapsulesConfig):\n",
    "    # Set batch size to 1 since we'll be running inference on\n",
    "    # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "config = InferenceConfig()\n",
    "##config.display()\n",
    "\n",
    "# Create model object in inference mode.\n",
    "model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR, config=config)\n",
    "\n",
    "# Load weights trained on MS-COCO\n",
    "model.load_weights(CAPSULES_MODEL_PATH, by_name=True)\n",
    "\n",
    "# Load dataset\n",
    "# Get the dataset from the releases page\n",
    "# https://github.com/matterport/Mask_RCNN/releases\n",
    "dataset = capsules.CapsulesDataset()\n",
    "dataset.load_capsules(Capsules_DIR, \"train\")\n",
    "\n",
    "# Must call before using the dataset\n",
    "dataset.prepare()\n",
    "\n",
    "print(\"Image Count: {}\".format(len(dataset.image_ids)))\n",
    "print(\"Class Count: {}\".format(dataset.num_classes))\n",
    "    \n",
    "class_names = dataset.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_to_camera(pixel, intrin, depth):\n",
    "\n",
    "    X = (pixel[0]-intrin[0]) * depth / intrin[2]\n",
    "    Y = (pixel[1]-intrin[1]) * depth / intrin[3]\n",
    "    return [X, Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_detect = 0\n",
    "img_index = 0\n",
    "def img_index_callback(data):\n",
    "    global img_index\n",
    "    global is_detect\n",
    "    \n",
    "    print(data.data)\n",
    "    img_index = data.data\n",
    "    is_detect = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rospy.Subscriber('/stone_img_index', String, img_index_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_center(mask):\n",
    "    g = np.mgrid[0:(mask.shape[0]),0:(mask.shape[1])]\n",
    "    multiple_ = np.stack([mask,mask],0)*g\n",
    "    total_sum = np.sum(multiple_,axis = (1,2))\n",
    "    total_number = np.sum(mask)\n",
    "    average = total_sum/total_number\n",
    "\n",
    "    return average.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_depth(mask,depth_im, depth_value_percentage_taken = 25.0):   \n",
    "    \n",
    "    masked_depth_im = depth_im * mask\n",
    "    #plt.imshow(masked_depth_im)\n",
    "    #plt.show()\n",
    "    depth_list = masked_depth_im[masked_depth_im > 0]\n",
    "    lower_bound = np.percentile(depth_list, 50 - depth_value_percentage_taken/2)\n",
    "    upper_bound = np.percentile(depth_list, 50 + depth_value_percentage_taken/2)\n",
    "    \n",
    "    cut_depth_list1 = depth_list[depth_list >= lower_bound]\n",
    "    cut_depth_list2 = cut_depth_list1[cut_depth_list1 <= upper_bound]\n",
    "    \n",
    "    depth_cut_average = np.average(cut_depth_list2)\n",
    "\n",
    "    \n",
    "    depth_max = np.amin(cut_depth_list2)/1000.0\n",
    "    depth = depth_cut_average/1000.0\n",
    "    \n",
    "    \n",
    "    \n",
    "    return depth, depth_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_mask(im,depth_im,r,center_point,distance_axis):\n",
    "    \n",
    "    mask_store = 10\n",
    "\n",
    "    print('center point',center_point)\n",
    "    center_y, center_x = center_point\n",
    "    \n",
    "    mask_size = np.sum(r['masks'],axis = (0,1))\n",
    "    \n",
    "    mask_index = np.argsort(np.array(mask_size))[-min(mask_store,r['masks'].shape[2]):]\n",
    "    \n",
    "    print('mask_size[mask_index]',mask_size[mask_index])\n",
    "    \n",
    "    min_distance2 = 10000000000\n",
    "    \n",
    "    most_center_point = []\n",
    "    \n",
    "    most_center_mask = []\n",
    "    \n",
    "    most_center_depth = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    for m in mask_index:\n",
    "        if mask_size[m] < 2000 and mask_size[m] > 1600 : #>1700\n",
    "        \n",
    "            mask = r['masks'][:,:,m]\n",
    "            mask = mask.astype(np.uint8)\n",
    "            \n",
    "\n",
    "            ## calculate center\n",
    "\n",
    "            [contours,hierarchy] = cv2.findContours(mask,cv2.RETR_TREE,cv2.CHAIN_APPROX_NONE)\n",
    "            cnt = contours[0]\n",
    "            M = cv2.moments(cnt)\n",
    "            \n",
    "            [center_point_y,center_point_x] = find_center(mask)\n",
    "            \n",
    "\n",
    "            distance2 = (center_point_x - center_x) **2 *distance_axis[1] + (\n",
    "                center_point_y -center_y) **2 *distance_axis[0]\n",
    "\n",
    "\n",
    "            #print(' mask')\n",
    "            #print('distance2',distance2)\n",
    "            #plt.imshow(mask)\n",
    "            #plt.show()\n",
    "\n",
    "\n",
    "\n",
    "            if distance2 < min_distance2:\n",
    "                \n",
    "\n",
    "                depth, depth_max1 = find_depth(mask,depth_im, depth_value_percentage_taken = 25.0)\n",
    "                depth2, depth_max = find_depth(mask,depth_im, depth_value_percentage_taken = 75.0)\n",
    "\n",
    "\n",
    "                #print('mask')\n",
    "                plt.imshow(mask)\n",
    "                #plt.show()\n",
    "\n",
    "\n",
    "                kernel = cv2.getStructuringElement(cv2.MORPH_RECT,(50, 50))\n",
    "                mask_dilated = cv2.dilate(mask, kernel)\n",
    "\n",
    "\n",
    "                #print('mask_dilated')\n",
    "                plt.imshow(mask_dilated)\n",
    "                #plt.show()\n",
    "\n",
    "                depth_env, depth_max_env = find_depth(mask_dilated,depth_im, depth_value_percentage_taken = 25.0)\n",
    "\n",
    "                print('depth',depth)\n",
    "                print(' depth_max_env', depth_max_env)\n",
    "                #if depth_max_env > depth - 0.005 and depth_max_env < depth +0.005: #depth_max_env > depth - 0.002:\n",
    "                if depth_max_env > depth - 0.007 :# - 0.002 :#and  depth_max - depth  < 0.01:\n",
    "                    print('record')\n",
    "                    min_distance2 = distance2 \n",
    "                    most_center_point = [center_point_y,center_point_x]\n",
    "                    most_center_mask = mask\n",
    "\n",
    "                    #most_center_depth_max =  depth_max\n",
    "                    most_center_depth = [depth, depth_max]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(' most_center_mask')\n",
    "    plt.imshow(most_center_mask)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    img_copy = im.copy()\n",
    "\n",
    "    cv2.circle(img_copy, (most_center_point[1], most_center_point[0]), 8, (255, 0, 255), 8)\n",
    "    \n",
    "    #print('img_copy')\n",
    "    #plt.imshow(img_copy)\n",
    "    #plt.show()\n",
    "    \n",
    "    [contours,hierarchy] = cv2.findContours(most_center_mask,cv2.RETR_TREE,cv2.CHAIN_APPROX_NONE)\n",
    "    cnt = contours[0]\n",
    "    for j in range(len(contours)):\n",
    "        if(len(contours[j]) > len(cnt)):\n",
    "            cnt = contours[j]\n",
    "    hull = cv2.convexHull(cnt,returnPoints = True)\n",
    "    rect = cv2.minAreaRect(hull)\n",
    "    box = cv2.boxPoints(rect)\n",
    "    box = np.int0(box)\n",
    "    #img_copy = im.copy()\n",
    "    cv2.drawContours(img_copy,[box],0,(0,0,255),2)\n",
    "    plt.imshow(img_copy)\n",
    "    plt.show()\n",
    "    print(box)\n",
    "    \n",
    "\n",
    "    return most_center_mask,most_center_point,most_center_depth,box #most_center_depth = [depth,depth_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pose(depth_im, mask, center,depth,mask_box):\n",
    "    \n",
    "    plt.imshow((depth_im*mask) != 0)\n",
    "    plt.show()\n",
    "    \n",
    "    depth_intrin = [321.8862609863281, 238.18316650390625, 612.0938720703125, 611.785888671875] # cx, cy, fx, fy\n",
    "    \n",
    "    position = pixel_to_camera(center[::-1], depth_intrin, depth[0] )\n",
    "    \n",
    "    if(np.linalg.norm(mask_box[0]-mask_box[1]) > np.linalg.norm(mask_box[1]-mask_box[2])):\n",
    "        rotation = math.atan2((mask_box[2]-mask_box[1])[1], (mask_box[2]-mask_box[1])[0])\n",
    "        print(\"rotation 111\")\n",
    "    else:\n",
    "        rotation = math.atan2((mask_box[1]-mask_box[0])[1], (mask_box[1]-mask_box[0])[0])\n",
    "        print(\"rotation 222\")\n",
    "    \n",
    "    depth_pixel = find_center(depth_im == depth[1]*1000)\n",
    "    \n",
    "    print('depth',depth)\n",
    "    print('depth_pixel',depth_pixel)\n",
    "    print('center',center)\n",
    "    \n",
    "    \n",
    "    direction_max = np.array(depth_pixel) - np.array(center)\n",
    "    \n",
    "    angle_max = math.atan2(direction_max[1],direction_max[0])\n",
    "    \n",
    "    print(\"angle_max\",math.degrees(angle_max))\n",
    "    \n",
    "    \n",
    "   \n",
    "    rotation = -rotation\n",
    "    \n",
    "    \n",
    "    angle_difference = abs(angle_max - rotation)\n",
    "    \n",
    "    if angle_difference > 2*math.pi:  ### reduce more >360 to between 0 to 360\n",
    "        angle_difference = angle_difference - 2* math.pi\n",
    "        print(\"angle_difference 111\")\n",
    "       \n",
    "    print('angle_difference,',math.degrees(angle_difference))\n",
    "    \n",
    "    if angle_difference > math.pi / 2.0 and angle_difference < 3 * math.pi / 2.0 : ## change gripper angle by 180 deg if it does not align with the correct height side\n",
    "        rotation = rotation - math.pi\n",
    "        print(\"rotation 211\")\n",
    "    elif rotation > math.pi:   ### reduce 180> x >360 to -180<x<0\n",
    "        rotation = rotation - 2 * math.pi\n",
    "        print(\"rotation 212\")\n",
    "   \n",
    "    \n",
    "    rotation = rotation + math.pi   \n",
    "\n",
    "    \n",
    "    if rotation > math.pi:  ## reduce 180> x >360 to -180<x<0\n",
    "        rotation = rotation - 2 * math.pi\n",
    "    \n",
    "    rotation = -rotation\n",
    "    \n",
    "    #print('rotation,',math.degrees(rotation))\n",
    "    \n",
    "    pose={\n",
    "            'x':position[0],\n",
    "            'y':position[1],\n",
    "            'z':depth[0] ,\n",
    "            'yaw':rotation,\n",
    "            'pitch':0,\n",
    "            'normal':0\n",
    "    }\n",
    "    \n",
    "    return pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_index = 0\n",
    "is_detect = 0\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "while (True):\n",
    "    if(is_detect == 1):\n",
    "        \n",
    "        pose_pub = rospy.Publisher('/stone_pose', stone_pose, queue_size=10)\n",
    "        \n",
    "        img_index = str(img_index)\n",
    "              \n",
    "        \n",
    "        # instance segmentation\n",
    "        image = skimage.io.imread(\"../samples/stones/JPEGImages/\"+img_index+\".jpeg\")\n",
    "        \n",
    "        depth_image = cv2.imread(\"../samples/stones/depth/\"+img_index+\".jpeg\")\n",
    "        depth_array = np.load(\"../samples/stones/depth/\"+img_index+\".npy\")\n",
    "    \n",
    "        \n",
    "        t = time.time()\n",
    "        # Run detection\n",
    "        results = model.detect([image], verbose=1)\n",
    "        print('MaskRCNN time',time.time() - t)\n",
    "        \n",
    "        # Visualize results\n",
    "        r = results[0]\n",
    "\n",
    "        \n",
    "        full_mask = np.sum(r['masks'],axis = 2)\n",
    "        print('full_mask.shape',full_mask.shape)\n",
    "        center = find_center(full_mask)\n",
    "        \n",
    "        \n",
    "        img_copy = image.copy()\n",
    "        cv2.circle(img_copy, (int(center[1]), int(center[0])), 8, (255, 0, 255), 8)\n",
    "        \n",
    "        \n",
    "        visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], \n",
    "                                    class_names, r['scores'])\n",
    "        \n",
    "        \n",
    "        visualize.display_mask(image, r['masks'])\n",
    "\n",
    "        #print('img_copy2')\n",
    "        #plt.imshow(img_copy)\n",
    "        #plt.show()\n",
    "        \n",
    "        \n",
    "        depth_image2 = skimage.io.imread(\"../samples/stones/depth/\"+img_index+\".jpeg\")\n",
    "        \n",
    "        mask_max, most_center_point, depth,box = find_mask(image,depth_image2 , r,center,[1,1])\n",
    "        \n",
    "        \n",
    "        pose2 = find_pose(depth_image2 , mask_max, most_center_point, depth, box )\n",
    "        \n",
    "        print('finished')\n",
    "        stone_pose_msg = stone_pose()\n",
    "        stone_pose_msg.x = pose2['x']\n",
    "        stone_pose_msg.y = pose2['y']\n",
    "        stone_pose_msg.z = pose2['z']\n",
    "        stone_pose_msg.yaw = pose2['yaw']\n",
    "        stone_pose_msg.pitch = 0 #pose['pitch']\n",
    "        stone_pose_msg.normal = [0,0,0]#pose['normal']\n",
    "\n",
    "        print('most_center_point',most_center_point)\n",
    "        pose_pub.publish(stone_pose_msg)\n",
    "        print('stone_pose_msg is ', stone_pose_msg)\n",
    "\n",
    "        \n",
    "        is_detect = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
