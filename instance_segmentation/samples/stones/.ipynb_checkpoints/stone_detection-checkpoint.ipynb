{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyrealsense2 as rs\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "import scipy\n",
    "import random\n",
    "import math\n",
    "import skimage.io\n",
    "import datetime\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../../\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "# Import COCO config\n",
    "#sys.path.append(os.path.join(ROOT_DIR, \"samples/balloon/\"))  # To find local version\n",
    "Stones_DIR = os.path.join(ROOT_DIR, \"datasets/stones\")\n",
    "\n",
    "import coco\n",
    "\n",
    "#from samples.blister import blister_mul_class\n",
    "import stones\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "STONES_MODEL_PATH = \"../../mask_rcnn_stones_0030.h5\"\n",
    "\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)\n",
    "\n",
    "# Directory of images to run detection on\n",
    "IMAGE_DIR = os.path.join(ROOT_DIR, \"images\")\n",
    "\n",
    "from poke_grasp.msg import stone_pose\n",
    "import rospy\n",
    "import geometry_msgs.msg\n",
    "import time\n",
    "import actionlib\n",
    "from std_msgs.msg import String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rospy.init_node('stone_segmentation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_directories():\n",
    "    if not os.path.exists(\"JPEGImages/\"):\n",
    "        os.makedirs(\"JPEGImages/\")\n",
    "    if not os.path.exists(\"depth/\"):\n",
    "        os.makedirs(\"depth/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_process(img_index):\n",
    "    make_directories()\n",
    "\n",
    "    # Setup:\n",
    "    pipeline = rs.pipeline()\n",
    "    config = rs.config()\n",
    "    config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "    config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "    config.enable_record_to_file('frames/0.bag')\n",
    "    profile = pipeline.start(config)\n",
    "\n",
    "    # Skip 5 first frames to give the Auto-Exposure time to adjust\n",
    "    for x in range(10):\n",
    "        pipeline.wait_for_frames()\n",
    "    # Store next frameset for later processing:\n",
    "    frames = pipeline.wait_for_frames()\n",
    "    color_frame = frames.get_color_frame()\n",
    "    depth_frame = frames.get_depth_frame()\n",
    "\n",
    "    # Cleanup:\n",
    "    pipeline.stop()\n",
    "    print(\"Frames Captured\")\n",
    "    \n",
    "    color = np.asanyarray(color_frame.get_data())\n",
    "    # Visualization\n",
    "    #plt.rcParams[\"axes.grid\"] = False\n",
    "    #plt.rcParams['figure.figsize'] = [12, 6]\n",
    "    #plt.imshow(color)\n",
    "    \n",
    "    colorizer = rs.colorizer()\n",
    "    colorized_depth = np.asanyarray(colorizer.colorize(depth_frame).get_data())\n",
    "    # Visualization\n",
    "    #plt.imshow(colorized_depth)\n",
    "    \n",
    "    # Create alignment primitive with color as its target stream:\n",
    "    align = rs.align(rs.stream.color)\n",
    "    frames = align.process(frames)\n",
    "\n",
    "    # Update color and depth frames:\n",
    "    aligned_depth_frame = frames.get_depth_frame()\n",
    "    #print(aligned_depth_frame)\n",
    "    colorized_depth = np.asanyarray(colorizer.colorize(aligned_depth_frame).get_data())\n",
    "\n",
    "    # Show the two frames together:\n",
    "    images = np.hstack((color, colorized_depth))\n",
    "    plt.imshow(images)\n",
    "\n",
    "    filecolor= \"JPEGImages/\"+img_index+\".jpg\"\n",
    "    filedepth= \"depth/\"+img_index+\".png\"\n",
    "    cv2.imwrite(filecolor, color)\n",
    "    cv2.imwrite(filedepth, colorized_depth)\n",
    "    \n",
    "    return colorized_depth, aligned_depth_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Count: 90\n",
      "Class Count: 2\n"
     ]
    }
   ],
   "source": [
    "# Start instance segmentation by Mask RCNN\n",
    "class InferenceConfig(stones.StonesConfig):\n",
    "    # Set batch size to 1 since we'll be running inference on\n",
    "    # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "config = InferenceConfig()\n",
    "##config.display()\n",
    "\n",
    "# Create model object in inference mode.\n",
    "model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR, config=config)\n",
    "\n",
    "# Load weights trained on MS-COCO\n",
    "model.load_weights(STONES_MODEL_PATH, by_name=True)\n",
    "\n",
    "# Load dataset\n",
    "# Get the dataset from the releases page\n",
    "# https://github.com/matterport/Mask_RCNN/releases\n",
    "dataset = stones.StonesDataset()\n",
    "dataset.load_stones(Stones_DIR, \"train\")\n",
    "\n",
    "# Must call before using the dataset\n",
    "dataset.prepare()\n",
    "\n",
    "print(\"Image Count: {}\".format(len(dataset.image_ids)))\n",
    "print(\"Class Count: {}\".format(dataset.num_classes))\n",
    "    \n",
    "class_names = dataset.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncolor_copy = color.copy()\\ncv2.circle(color_copy,(350,120), 5, (0,0,0), 5)\\ncv2.circle(color_copy,(200,100), 5, (0,0,0), 5)\\nplt.imshow(color_copy)\\n\\nprint(aligned_depth_frame.get_distance(350,120))\\nprint(aligned_depth_frame.get_distance(200,100))\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(color.shape)\n",
    "#print(r['masks'].shape)\n",
    "#print(r['scores'])\n",
    "\n",
    "'''\n",
    "color_copy = color.copy()\n",
    "cv2.circle(color_copy,(350,120), 5, (0,0,0), 5)\n",
    "cv2.circle(color_copy,(200,100), 5, (0,0,0), 5)\n",
    "plt.imshow(color_copy)\n",
    "\n",
    "print(aligned_depth_frame.get_distance(350,120))\n",
    "print(aligned_depth_frame.get_distance(200,100))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute position of mask center and rotation of mask\n",
    "def get_mask_pose(depth_intrin, m_c, mm_pair, max_d, min_d):\n",
    "    #position = m_c\n",
    "    print(m_c)\n",
    "    m_c_dist = aligned_depth_frame.get_distance(m_c[0], m_c[1])\n",
    "    position = rs.rs2_deproject_pixel_to_point(depth_intrin, m_c, m_c_dist)\n",
    "    # Counterclockwise is positive direction\n",
    "    yaw = math.atan2(mm_pair[0][0]-mm_pair[1][0], mm_pair[0][1]-mm_pair[1][1])\n",
    "    pitch = math.atan2(max_d-min_d, 0.023)\n",
    "    if yaw > 0:\n",
    "        if mm_pair[0][0] > mm_pair[1][0]:\n",
    "            yaw = math.pi - yaw\n",
    "    else:\n",
    "        if mm_pair[0][0] < mm_pair[1][0]:\n",
    "            yaw = math.pi + yaw\n",
    "    pose={\n",
    "        'x':position[0],\n",
    "        'y':position[1],\n",
    "        'yaw':yaw,\n",
    "        'pitch':pitch\n",
    "    }\n",
    "    return pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def generate_stone_pose(aligned_depth_frame, colorized_depth, seg_result):\n",
    "    depth_copy = colorized_depth.copy()\n",
    "    mask_size = []\n",
    "\n",
    "    # Rank masks by footprints\n",
    "    for m in range(seg_result['masks'].shape[2]):\n",
    "        mask = seg_result['masks'][:,:,m]\n",
    "        #print(mask.shape)\n",
    "        mask = mask.astype(np.uint8)\n",
    "        mask_size.append(np.sum(mask))\n",
    "    mask_index = np.argsort(np.array(mask_size))[-3:seg_result['masks'].shape[2]]\n",
    "    #print(np.argsort(np.array(mask_size)))\n",
    "    #print(np.argsort(np.array(mask_size))[-3:r['masks'].shape[2]])\n",
    "\n",
    "    for m in mask_index:\n",
    "        mask = seg_result['masks'][:,:,m]\n",
    "        mask = mask.astype(np.uint8)\n",
    "    #    result = colorized_depth.copy()\n",
    "    #    result[mask!=0] = (0,0,255)\n",
    "\n",
    "        # Detect edge for each mask\n",
    "        edges = cv2.Canny(mask,0,1)\n",
    "    #        plt.figure()\n",
    "    #        plt.subplot(121),plt.imshow(mask, cmap = 'gray')\n",
    "    #        plt.subplot(122),plt.imshow(edges, cmap = 'gray')\n",
    "        plt.imshow(edges)\n",
    "        #print(edges)\n",
    "\n",
    "        distance = []\n",
    "        depth_point = []\n",
    "        edge_point = []\n",
    "        min_candidate = []\n",
    "        max_candidate = []\n",
    "        count = 0\n",
    "        dist_max = -100000\n",
    "        max_index = [0, 0]\n",
    "        dist_min = 100000\n",
    "        min_index = [0, 0]\n",
    "        depth_intrin = aligned_depth_frame.profile.as_video_stream_profile().intrinsics\n",
    "        print(depth_intrin)\n",
    "        overall_mask_x = []\n",
    "        overall_mask_y = []\n",
    "        for i in range(depth_copy.shape[0]):\n",
    "            for j in range(depth_copy.shape[1]):\n",
    "                if edges[i,j] == 255:\n",
    "                #if mask[i, j] == 1:\n",
    "                    overall_mask_x.append(j)\n",
    "                    overall_mask_y.append(i)\n",
    "                    depth_copy[i,j] = [255, 255, 255]\n",
    "                    edge_point.append([j, i])\n",
    "                    # get_distance: Input: (x,y) in pixel coordinate, opposite from index of image matrix\n",
    "                    distance.append(aligned_depth_frame.get_distance(j,i))\n",
    "                    #print('distance', distance)\n",
    "                    # map pixel to 3d point\n",
    "                    depth_point.append(rs.rs2_deproject_pixel_to_point(depth_intrin, [j, i], distance[count]))\n",
    "\n",
    "                    if aligned_depth_frame.get_distance(j,i) > dist_max and aligned_depth_frame.get_distance(j,i) != 0:\n",
    "                        dist_max = aligned_depth_frame.get_distance(j,i)\n",
    "                        #print(\"dist_max\", dist_max)\n",
    "                        max_index = [j, i]\n",
    "\n",
    "                    if aligned_depth_frame.get_distance(j,i) < dist_min and aligned_depth_frame.get_distance(j,i) != 0:\n",
    "                        dist_min = aligned_depth_frame.get_distance(j,i)\n",
    "                        #print(\"dist_min\", dist_min)\n",
    "                        min_index = [j, i]\n",
    "\n",
    "                    count = count + 1\n",
    "        #    print(\"max_index\", max_index)\n",
    "        #    print(\"min_index\", min_index)\n",
    "\n",
    "\n",
    "        #    print(int(round(np.mean(overall_mask_x))))\n",
    "        #    print(int(round(np.mean(overall_mask_y))))\n",
    "\n",
    "        mask_center = [int(round(np.mean(overall_mask_x))), int(round(np.mean(overall_mask_y)))]\n",
    "        #print(mask_center)\n",
    "        maxp2cp_vec = np.array(max_index) - np.array(mask_center)\n",
    "        minp2cp_vec = np.array(min_index) - np.array(mask_center)\n",
    "        dth_maxp_min = 10000\n",
    "        dth_minp_min = 10000\n",
    "        for i in range(len(edge_point)):\n",
    "            cp2fakep_vec = np.array(mask_center) - np.array(edge_point[i])\n",
    "            #print(np.matmul(maxp2cp_vec,cp2fakep_vec) / (np.linalg.norm(maxp2cp_vec)*np.linalg.norm(cp2fakep_vec)))\n",
    "            dth_maxp = np.arccos(np.matmul(maxp2cp_vec,cp2fakep_vec) / (np.linalg.norm(maxp2cp_vec)*np.linalg.norm(cp2fakep_vec)))\n",
    "            if  dth_maxp < dth_maxp_min:\n",
    "                dth_maxp_min = dth_maxp\n",
    "                min_candidate = edge_point[i]\n",
    "            #print(np.matmul(minp2cp_vec,cp2fakep_vec) / (np.linalg.norm(minp2cp_vec)*np.linalg.norm(cp2fakep_vec)))\n",
    "            dth_minp = np.arccos(np.matmul(minp2cp_vec,cp2fakep_vec) / (np.linalg.norm(minp2cp_vec)*np.linalg.norm(cp2fakep_vec)))\n",
    "            if dth_minp < dth_minp_min:\n",
    "                dth_minp_min = dth_minp\n",
    "                max_candidate = edge_point[i]\n",
    "        #    print(\"min_candidate\", min_candidate)\n",
    "        #    print(\"max_candidate\", max_candidate)\n",
    "        max_dist_gap = aligned_depth_frame.get_distance(max_candidate[0],max_candidate[1]) \\\n",
    "        -aligned_depth_frame.get_distance(max_index[0],max_index[1])\n",
    "        print('max_dist_gap', max_dist_gap)\n",
    "        min_dist_gap = aligned_depth_frame.get_distance(min_candidate[0],min_candidate[1]) \\\n",
    "        -aligned_depth_frame.get_distance(min_index[0],min_index[1])\n",
    "        if  max_dist_gap < min_dist_gap:\n",
    "            max_min_pair = [max_candidate, min_index]\n",
    "        else:\n",
    "            max_min_pair = [max_index, min_candidate]\n",
    "        dist_min = aligned_depth_frame.get_distance(max_min_pair[1][0],max_min_pair[1][1])\n",
    "        dist_max = aligned_depth_frame.get_distance(max_min_pair[0][0],max_min_pair[0][1])\n",
    "        print('dist_max',dist_max, 'dist_min',dist_min)\n",
    "        ############## TEST FILTER #################\n",
    "        '''\n",
    "        #print(distance)\n",
    "        dist_min = []\n",
    "        distance.sort()\n",
    "        #print(distance)\n",
    "        for i in range(5):\n",
    "            dist_min.append(distance[i])\n",
    "        dist_min = np.mean(dist_min)\n",
    "        print(\"mean_dist_min\", dist_min )\n",
    "        min_real_mean_gap = 10000\n",
    "        for i in range(depth_copy.shape[0]):\n",
    "            for j in range(depth_copy.shape[1]):\n",
    "                if mask[i,j] == 1:\n",
    "                    #if aligned_depth_frame.get_distance(j,i) == dist_min:\n",
    "                        #cv2.circle(depth_copy,(j,i), 5, (255,0,0), 1)\n",
    "                    if aligned_depth_frame.get_distance(j,i)-dist_min < min_real_mean_gap:\n",
    "                        min_real_mean_gap = aligned_depth_frame.get_distance(j,i)-dist_min\n",
    "                        min_index = [j, i]\n",
    "        print(\"min_index\", min_index)\n",
    "        '''\n",
    "        ############################################\n",
    "\n",
    "        #print(\"points\", depth_point)\n",
    "        cv2.circle(depth_copy,tuple(max_min_pair[0]), 5, (0,0,0), 2)\n",
    "        cv2.circle(depth_copy,tuple(max_min_pair[1]), 5, (255,0,0), 2)\n",
    "        cv2.circle(depth_copy,tuple(mask_center), 5, (255,0,0), 1)\n",
    "\n",
    "        print(get_mask_pose(depth_intrin, mask_center, max_min_pair, dist_max, dist_min))\n",
    "    #cv2.circle(depth_copy,(100,200), 5, (0,0,0), 10)\n",
    "    plt.figure()\n",
    "    plt.rcParams['figure.figsize'] = [24, 12]\n",
    "    plt.imshow(depth_copy)\n",
    "    \n",
    "    return get_mask_pose(depth_intrin, mask_center, max_min_pair, dist_max, dist_min), depth_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_detect = 0\n",
    "img_index = 0\n",
    "def img_index_callback(data):\n",
    "    global img_index\n",
    "    global is_detect\n",
    "    \n",
    "    print(data.data)\n",
    "    img_index = data.data\n",
    "    is_detect = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<rospy.topics.Subscriber at 0x7ff3e836d860>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "rospy.Subscriber('/stone_img_index', String, img_index_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_detect = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "xioctl(VIDIOC_S_FMT) failed Last Error: Device or resource busy",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-ef7a6d302c58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m640\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m480\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbgr8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_record_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'frames/0.bag'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mprofile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Skip 5 first frames to give the Auto-Exposure time to adjust\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: xioctl(VIDIOC_S_FMT) failed Last Error: Device or resource busy"
     ]
    }
   ],
   "source": [
    "while (True):\n",
    "    if(is_detect == 1):\n",
    "        \n",
    "        pose_pub = rospy.Publisher('/stone_pose', stone_pose, queue_size=10)\n",
    "        \n",
    "        img_index = str(img_index)\n",
    "        \n",
    "        #colorized_depth, aligned_depth_frame = image_process(img_index)\n",
    "        make_directories()\n",
    "\n",
    "        # Setup:\n",
    "        pipeline = rs.pipeline()\n",
    "        config = rs.config()\n",
    "        config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "        config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "        config.enable_record_to_file('frames/0.bag')\n",
    "        profile = pipeline.start(config)\n",
    "\n",
    "        # Skip 5 first frames to give the Auto-Exposure time to adjust\n",
    "        for x in range(10):\n",
    "            pipeline.wait_for_frames()\n",
    "        # Store next frameset for later processing:\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        color_frame = frames.get_color_frame()\n",
    "        depth_frame = frames.get_depth_frame()\n",
    "\n",
    "        # Cleanup:\n",
    "        pipeline.stop()\n",
    "        print(\"Frames Captured\")\n",
    "\n",
    "        color = np.asanyarray(color_frame.get_data())\n",
    "        # Visualization\n",
    "        #plt.rcParams[\"axes.grid\"] = False\n",
    "        #plt.rcParams['figure.figsize'] = [12, 6]\n",
    "        #plt.imshow(color)\n",
    "\n",
    "        colorizer = rs.colorizer()\n",
    "        colorized_depth = np.asanyarray(colorizer.colorize(depth_frame).get_data())\n",
    "        # Visualization\n",
    "        #plt.imshow(colorized_depth)\n",
    "\n",
    "        # Create alignment primitive with color as its target stream:\n",
    "        align = rs.align(rs.stream.color)\n",
    "        frames = align.process(frames)\n",
    "\n",
    "        # Update color and depth frames:\n",
    "        aligned_depth_frame = frames.get_depth_frame()\n",
    "        #print(aligned_depth_frame)\n",
    "        colorized_depth = np.asanyarray(colorizer.colorize(aligned_depth_frame).get_data())\n",
    "\n",
    "        # Show the two frames together:\n",
    "        images = np.hstack((color, colorized_depth))\n",
    "        plt.imshow(images)\n",
    "\n",
    "        filecolor= \"JPEGImages/\"+img_index+\".jpg\"\n",
    "        filedepth= \"depth/\"+img_index+\".png\"\n",
    "        cv2.imwrite(filecolor, color)\n",
    "        cv2.imwrite(filedepth, colorized_depth)\n",
    "        \n",
    "        \n",
    "        # instance segmentation\n",
    "        image = scipy.misc.imread(\"JPEGImages/\"+img_index+\".jpg\")\n",
    "        \n",
    "        # Run detection\n",
    "        results = model.detect([image], verbose=1)\n",
    "        \n",
    "        # Visualize results\n",
    "        r = results[0]\n",
    "        \n",
    "        \n",
    "        pose, depth_copy = generate_stone_pose(aligned_depth_frame, colorized_depth, r)\n",
    "        #plt.figure()\n",
    "        #plt.rcParams['figure.figsize'] = [24, 12]\n",
    "        #plt.imshow(depth_copy)\n",
    "        \n",
    "        visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], \n",
    "                                    class_names, r['scores'])\n",
    "        \n",
    "        stone_pose_msg = stone_pose()\n",
    "        stone_pose_msg.x = pose['x']\n",
    "        stone_pose_msg.y = pose['y']\n",
    "        stone_pose_msg.yaw = pose['yaw']\n",
    "        stone_pose_msg.pitch = pose['pitch']\n",
    "\n",
    "        pose_pub.publish(stone_pose_msg)\n",
    "        print('stone_pose_msg is ', stone_pose_msg)\n",
    "        \n",
    "        is_detect = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
