{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyrealsense2 as rs\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "import scipy\n",
    "import random\n",
    "import math\n",
    "import skimage.io\n",
    "import datetime\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../../\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "# Import COCO config\n",
    "#sys.path.append(os.path.join(ROOT_DIR, \"samples/balloon/\"))  # To find local version\n",
    "Stones_DIR = os.path.join(ROOT_DIR, \"datasets/stones\")\n",
    "\n",
    "import coco\n",
    "\n",
    "#from samples.blister import blister_mul_class\n",
    "import stones\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "STONES_MODEL_PATH = \"../../mask_rcnn_stones_0030.h5\"\n",
    "\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)\n",
    "\n",
    "# Directory of images to run detection on\n",
    "IMAGE_DIR = os.path.join(ROOT_DIR, \"images\")\n",
    "\n",
    "from poke_grasp.msg import stone_pose\n",
    "import rospy\n",
    "import geometry_msgs.msg\n",
    "import time\n",
    "import actionlib\n",
    "from std_msgs.msg import String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rospy.init_node('stone_segmentation_rs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_directories():\n",
    "    if not os.path.exists(\"JPEGImages/\"):\n",
    "        os.makedirs(\"JPEGImages/\")\n",
    "    if not os.path.exists(\"depth/\"):\n",
    "        os.makedirs(\"depth/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_process(img_index):\n",
    "    make_directories()\n",
    "\n",
    "    # Setup:\n",
    "    pipeline = rs.pipeline()\n",
    "    config = rs.config()\n",
    "    config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "    config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "    config.enable_record_to_file('frames/0.bag')\n",
    "    profile = pipeline.start(config)\n",
    "\n",
    "    # Skip 5 first frames to give the Auto-Exposure time to adjust\n",
    "    for x in range(10):\n",
    "        pipeline.wait_for_frames()\n",
    "    # Store next frameset for later processing:\n",
    "    frames = pipeline.wait_for_frames()\n",
    "    color_frame = frames.get_color_frame()\n",
    "    depth_frame = frames.get_depth_frame()\n",
    "\n",
    "    # Cleanup:\n",
    "    pipeline.stop()\n",
    "    print(\"Frames Captured\")\n",
    "    \n",
    "    color = np.asanyarray(color_frame.get_data())\n",
    "    # Visualization\n",
    "    #plt.rcParams[\"axes.grid\"] = False\n",
    "    #plt.rcParams['figure.figsize'] = [12, 6]\n",
    "    #plt.imshow(color)\n",
    "    \n",
    "    colorizer = rs.colorizer()\n",
    "    colorized_depth = np.asanyarray(colorizer.colorize(depth_frame).get_data())\n",
    "    # Visualization\n",
    "    #plt.imshow(colorized_depth)\n",
    "    \n",
    "    # Create alignment primitive with color as its target stream:\n",
    "    align = rs.align(rs.stream.color)\n",
    "    frames = align.process(frames)\n",
    "\n",
    "    # Update color and depth frames:\n",
    "    aligned_depth_frame = frames.get_depth_frame()\n",
    "    #print(aligned_depth_frame)\n",
    "    colorized_depth = np.asanyarray(colorizer.colorize(aligned_depth_frame).get_data())\n",
    "\n",
    "    # Show the two frames together:\n",
    "    images = np.hstack((color, colorized_depth))\n",
    "    plt.imshow(images)\n",
    "\n",
    "    filecolor= \"JPEGImages/\"+img_index+\".jpg\"\n",
    "    filedepth= \"depth/\"+img_index+\".png\"\n",
    "    cv2.imwrite(filecolor, color)\n",
    "    cv2.imwrite(filedepth, colorized_depth)\n",
    "    \n",
    "    return colorized_depth, aligned_depth_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Count: 90\n",
      "Class Count: 2\n"
     ]
    }
   ],
   "source": [
    "# Start instance segmentation by Mask RCNN\n",
    "class InferenceConfig(stones.StonesConfig):\n",
    "    # Set batch size to 1 since we'll be running inference on\n",
    "    # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "config = InferenceConfig()\n",
    "##config.display()\n",
    "\n",
    "# Create model object in inference mode.\n",
    "model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR, config=config)\n",
    "\n",
    "# Load weights trained on MS-COCO\n",
    "model.load_weights(STONES_MODEL_PATH, by_name=True)\n",
    "\n",
    "# Load dataset\n",
    "# Get the dataset from the releases page\n",
    "# https://github.com/matterport/Mask_RCNN/releases\n",
    "dataset = stones.StonesDataset()\n",
    "dataset.load_stones(Stones_DIR, \"train\")\n",
    "\n",
    "# Must call before using the dataset\n",
    "dataset.prepare()\n",
    "\n",
    "print(\"Image Count: {}\".format(len(dataset.image_ids)))\n",
    "print(\"Class Count: {}\".format(dataset.num_classes))\n",
    "    \n",
    "class_names = dataset.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncolor_copy = color.copy()\\ncv2.circle(color_copy,(350,120), 5, (0,0,0), 5)\\ncv2.circle(color_copy,(200,100), 5, (0,0,0), 5)\\nplt.imshow(color_copy)\\n\\nprint(aligned_depth_frame.get_distance(350,120))\\nprint(aligned_depth_frame.get_distance(200,100))\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(color.shape)\n",
    "#print(r['masks'].shape)\n",
    "#print(r['scores'])\n",
    "\n",
    "'''\n",
    "color_copy = color.copy()\n",
    "cv2.circle(color_copy,(350,120), 5, (0,0,0), 5)\n",
    "cv2.circle(color_copy,(200,100), 5, (0,0,0), 5)\n",
    "plt.imshow(color_copy)\n",
    "\n",
    "print(aligned_depth_frame.get_distance(350,120))\n",
    "print(aligned_depth_frame.get_distance(200,100))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute position of mask center and rotation of mask\n",
    "def get_mask_pose(depth_intrin, m_c, mm_pair, max_d, min_d):\n",
    "    #position = m_c\n",
    "    print(m_c)\n",
    "    m_c_dist = aligned_depth_frame.get_distance(m_c[0], m_c[1])\n",
    "    position = rs.rs2_deproject_pixel_to_point(depth_intrin, m_c, m_c_dist)\n",
    "    # Counterclockwise is positive direction\n",
    "    yaw = math.atan2(mm_pair[0][0]-mm_pair[1][0], mm_pair[0][1]-mm_pair[1][1])\n",
    "    pitch = math.atan2(max_d-min_d, 0.023)\n",
    "    if yaw > 0:\n",
    "        if mm_pair[0][0] > mm_pair[1][0]:\n",
    "            yaw = math.pi - yaw\n",
    "    else:\n",
    "        if mm_pair[0][0] < mm_pair[1][0]:\n",
    "            yaw = math.pi + yaw\n",
    "    pose={\n",
    "        'x':position[0],\n",
    "        'y':position[1],\n",
    "        'yaw':yaw,\n",
    "        'pitch':pitch\n",
    "    }\n",
    "    return pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def sortFourth(val): \n",
    "    return val[3]  \n",
    "\n",
    "def generate_stone_pose(aligned_depth_frame, colorized_depth, seg_result):\n",
    "    depth_copy = colorized_depth.copy()\n",
    "    mask_size = []\n",
    "    mask_store = 3\n",
    "    window = 10\n",
    "\n",
    "    # Rank masks by footprints\n",
    "    for m in range(seg_result['masks'].shape[2]):\n",
    "        mask = seg_result['masks'][:,:,m]\n",
    "        #print(mask.shape)\n",
    "        mask = mask.astype(np.uint8)\n",
    "        mask_size.append(np.sum(mask))\n",
    "    mask_index = np.argsort(np.array(mask_size))[-3:seg_result['masks'].shape[2]]\n",
    "    #print(np.argsort(np.array(mask_size)))\n",
    "    #print(np.argsort(np.array(mask_size))[-3:r['masks'].shape[2]])\n",
    "    \n",
    "    mask_center = []\n",
    "    max_min_pair = []\n",
    "    dist_min = []\n",
    "    dist_max = []\n",
    "    count_m = 0    \n",
    "    for m in mask_index:\n",
    "        mask = seg_result['masks'][:,:,m]\n",
    "        mask = mask.astype(np.uint8)\n",
    "    #    result = colorized_depth.copy()\n",
    "    #    result[mask!=0] = (0,0,255)\n",
    "\n",
    "        # Detect edge for each mask\n",
    "        edges = cv2.Canny(mask,0,1)\n",
    "    #        plt.figure()\n",
    "    #        plt.subplot(121),plt.imshow(mask, cmap = 'gray')\n",
    "    #        plt.subplot(122),plt.imshow(edges, cmap = 'gray')\n",
    "        plt.imshow(edges)\n",
    "        #print(edges)\n",
    "\n",
    "        distance = []\n",
    "        depth_point = []\n",
    "        edge_point = []\n",
    "        min_candidate = []\n",
    "        max_candidate = []\n",
    "        count = 0\n",
    "        dist_max_ = -100000\n",
    "        max_index = [0, 0]\n",
    "        dist_min_ = 100000\n",
    "        min_index = [0, 0]\n",
    "        depth_intrin = aligned_depth_frame.profile.as_video_stream_profile().intrinsics\n",
    "        print(depth_intrin)\n",
    "        overall_mask_x = []\n",
    "        overall_mask_y = []\n",
    "        for i in range(depth_copy.shape[0]):\n",
    "            for j in range(depth_copy.shape[1]):\n",
    "                if edges[i,j] == 255:\n",
    "                #if mask[i, j] == 1:\n",
    "                    overall_mask_x.append(j)\n",
    "                    overall_mask_y.append(i)\n",
    "                    depth_copy[i,j] = [255, 255, 255]\n",
    "                    edge_point.append([j, i])\n",
    "                    # get_distance: Input: (x,y) in pixel coordinate, opposite from index of image matrix\n",
    "                    distance.append(aligned_depth_frame.get_distance(j,i))\n",
    "                    #print('distance', distance)\n",
    "                    # map pixel to 3d point\n",
    "                    depth_point.append(rs.rs2_deproject_pixel_to_point(depth_intrin, [j, i], distance[count]))\n",
    "\n",
    "                    if aligned_depth_frame.get_distance(j,i) > dist_max_ and aligned_depth_frame.get_distance(j,i) != 0:\n",
    "                        dist_max_ = aligned_depth_frame.get_distance(j,i)\n",
    "                        #print(\"dist_max\", dist_max)\n",
    "                        max_index = [j, i]\n",
    "\n",
    "                    if aligned_depth_frame.get_distance(j,i) < dist_min_ and aligned_depth_frame.get_distance(j,i) != 0:\n",
    "                        dist_min_ = aligned_depth_frame.get_distance(j,i)\n",
    "                        #print(\"dist_min\", dist_min)\n",
    "                        min_index = [j, i]\n",
    "\n",
    "                    count = count + 1\n",
    "                    \n",
    "        cv2.circle(depth_copy,tuple(edge_point[0]), 5, (0,0,255), 2)\n",
    "        print(len(edge_point))\n",
    "        avg = np.zeros(len(edge_point))\n",
    "        for i in range(len(edge_point)):\n",
    "            for j in range(window):\n",
    "                #print(i+j+len(edge_point)%len(edge_point))\n",
    "                edge_px = edge_point[(i+j+len(edge_point))%len(edge_point)]\n",
    "                avg[i] += depth_array[edge_px[1], edge_px[0]]/1000\n",
    "        avg = avg / window\n",
    "        print('avg', avg)\n",
    "        sorted_avg_idx = avg.argsort()\n",
    "        dist_min.append(avg[sorted_avg_idx[0]])\n",
    "        dist_max.append(avg[sorted_avg_idx[len(sorted_avg_idx)-1]])\n",
    "        print('dist_max',dist_max[count_m], 'dist_min',dist_min[count_m])\n",
    "        #print('sorted_avg_idx', sorted_avg_idx)sorted_avg_idx[len(sorted_avg_idx)-1]+int(window/2)\n",
    "        print('index', sorted_avg_idx[0]+int(window/2), sorted_avg_idx[len(sorted_avg_idx)-1]+int(window/2))\n",
    "        dist_min_px = edge_point[(sorted_avg_idx[0]+int(window/2)+len(edge_point))%len(edge_point)]\n",
    "        dist_max_px = edge_point[(sorted_avg_idx[len(sorted_avg_idx)-1]+int(window/2)+len(edge_point))%len(edge_point)]\n",
    "        max_min_pair.append([dist_max_px, dist_min_px])\n",
    "        print('max_min', max_min_pair[count_m])\n",
    "        \n",
    "        #    print(\"max_index\", max_index)\n",
    "        #    print(\"min_index\", min_index)\n",
    "\n",
    "\n",
    "        #    print(int(round(np.mean(overall_mask_x))))\n",
    "        #    print(int(round(np.mean(overall_mask_y))))\n",
    "\n",
    "        mask_center.append([int(round(np.mean(overall_mask_x))), int(round(np.mean(overall_mask_y))), count_m]) #in pixel frame\n",
    "        #print(mask_center)\n",
    "        '''\n",
    "        maxp2cp_vec = np.array(max_index) - np.array(mask_center)\n",
    "        minp2cp_vec = np.array(min_index) - np.array(mask_center)\n",
    "        dth_maxp_min = 10000\n",
    "        dth_minp_min = 10000\n",
    "        for i in range(len(edge_point)):\n",
    "            cp2fakep_vec = np.array(mask_center) - np.array(edge_point[i])\n",
    "            #print(np.matmul(maxp2cp_vec,cp2fakep_vec) / (np.linalg.norm(maxp2cp_vec)*np.linalg.norm(cp2fakep_vec)))\n",
    "            dth_maxp = np.arccos(np.matmul(maxp2cp_vec,cp2fakep_vec) / (np.linalg.norm(maxp2cp_vec)*np.linalg.norm(cp2fakep_vec)))\n",
    "            if  dth_maxp < dth_maxp_min:\n",
    "                dth_maxp_min = dth_maxp\n",
    "                min_candidate = edge_point[i]\n",
    "            #print(np.matmul(minp2cp_vec,cp2fakep_vec) / (np.linalg.norm(minp2cp_vec)*np.linalg.norm(cp2fakep_vec)))\n",
    "            dth_minp = np.arccos(np.matmul(minp2cp_vec,cp2fakep_vec) / (np.linalg.norm(minp2cp_vec)*np.linalg.norm(cp2fakep_vec)))\n",
    "            if dth_minp < dth_minp_min:\n",
    "                dth_minp_min = dth_minp\n",
    "                max_candidate = edge_point[i]\n",
    "        #    print(\"min_candidate\", min_candidate)\n",
    "        #    print(\"max_candidate\", max_candidate)\n",
    "        max_dist_gap = aligned_depth_frame.get_distance(max_candidate[0],max_candidate[1]) \\\n",
    "        -aligned_depth_frame.get_distance(max_index[0],max_index[1])\n",
    "        print('max_dist_gap', max_dist_gap)\n",
    "        min_dist_gap = aligned_depth_frame.get_distance(min_candidate[0],min_candidate[1]) \\\n",
    "        -aligned_depth_frame.get_distance(min_index[0],min_index[1])\n",
    "        if  max_dist_gap < min_dist_gap:\n",
    "            max_min_pair = [max_candidate, min_index]\n",
    "        else:\n",
    "            max_min_pair = [max_index, min_candidate]\n",
    "        dist_min = aligned_depth_frame.get_distance(max_min_pair[1][0],max_min_pair[1][1])\n",
    "        dist_max = aligned_depth_frame.get_distance(max_min_pair[0][0],max_min_pair[0][1])\n",
    "        print('dist_max',dist_max, 'dist_min',dist_min)\n",
    "        '''\n",
    "        #print(\"points\", depth_point)\n",
    "        cv2.circle(depth_copy,tuple(max_min_pair[count_m][0]), 5, (0,0,0), 2)\n",
    "        cv2.circle(depth_copy,tuple(max_min_pair[count_m][1]), 5, (255,0,0), 2)\n",
    "        cv2.circle(depth_copy,tuple(mask_center[count_m][:2]), 5, (255,0,0), 1)\n",
    "        print(get_mask_pose(depth_intrin, depth_array, mask_center[count_m], max_min_pair[mask_center[count_m][2]], dist_max[mask_center[count_m][2]], dist_min[mask_center[count_m][2]]))\n",
    "        count_m = count_m + 1\n",
    "\n",
    "        #print(get_mask_pose(depth_intrin, mask_center, max_min_pair, dist_max, dist_min))\n",
    "    #cv2.circle(depth_copy,(100,200), 5, (0,0,0), 10)\n",
    "    \n",
    "    image_center = [depth_copy.shape[1]/2, depth_copy.shape[0]/2]\n",
    "    for i in range(mask_store):\n",
    "        print('norm', mask_center[i][:2], image_center)\n",
    "        print('norm', np.linalg.norm(np.array(mask_center[i][:2])-np.array(image_center)))\n",
    "        mask_center[i].append(np.linalg.norm(np.array(mask_center[i][:2])-np.array(image_center)))\n",
    "    mask_center.sort(key=sortFourth)\n",
    "    plt.figure()\n",
    "    plt.rcParams['figure.figsize'] = [24, 12]\n",
    "    plt.imshow(depth_copy)\n",
    "    \n",
    "    \n",
    "    return get_mask_pose(depth_intrin, depth_array, mask_center[0], max_min_pair[mask_center[0][2]], dist_max[mask_center[0][2]], dist_min[mask_center[0][2]]), depth_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_detect = 0\n",
    "img_index = 0\n",
    "def img_index_callback(data):\n",
    "    global img_index\n",
    "    global is_detect\n",
    "    \n",
    "    print(data.data)\n",
    "    img_index = data.data\n",
    "    is_detect = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<rospy.topics.Subscriber at 0x7ff3e836d860>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "rospy.Subscriber('/stone_img_index', String, img_index_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_detect = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames Captured\n",
      "Processing 1 images\n",
      "image                    shape: (480, 640, 3)         min:    6.00000  max:  240.00000  uint8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhekai/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:64: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/home/zhekai/anaconda3/envs/tensorflow/lib/python3.6/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "molded_images            shape: (1, 1024, 1024, 3)    min: -123.70000  max:  102.10000  float64\n",
      "image_metas              shape: (1, 14)               min:    0.00000  max: 1024.00000  float64\n",
      "anchors                  shape: (1, 261888, 4)        min:   -0.35390  max:    1.29134  float32\n",
      "width: 640, height: 480, ppx: 315.37, ppy: 239.779, fx: 619.28, fy: 619.28, model: None, coeffs: [0, 0, 0, 0, 0]\n",
      "204\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'window' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-3cbe67c22441>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mpose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth_copy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_stone_pose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maligned_depth_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolorized_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0;31m#plt.figure()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;31m#plt.rcParams['figure.figsize'] = [24, 12]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-e8e01b9cbad8>\u001b[0m in \u001b[0;36mgenerate_stone_pose\u001b[0;34m(aligned_depth_frame, colorized_depth, seg_result)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mavg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_point\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_point\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m                 \u001b[0;31m#print(i+j+len(edge_point)%len(edge_point))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0medge_px\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0medge_point\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_point\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_point\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'window' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "while (True):\n",
    "    if(is_detect == 1):\n",
    "        \n",
    "        pose_pub = rospy.Publisher('/stone_pose', stone_pose, queue_size=10)\n",
    "        \n",
    "        #img_index = str(img_index)\n",
    "        \n",
    "        #colorized_depth, aligned_depth_frame = image_process(img_index)\n",
    "        make_directories()\n",
    "\n",
    "        # Setup:\n",
    "        pipeline = rs.pipeline()\n",
    "        config = rs.config()\n",
    "        config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "        config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "        config.enable_record_to_file('frames/0.bag')\n",
    "        profile = pipeline.start(config)\n",
    "\n",
    "        # Skip 5 first frames to give the Auto-Exposure time to adjust\n",
    "        for x in range(10):\n",
    "            pipeline.wait_for_frames()\n",
    "        # Store next frameset for later processing:\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        color_frame = frames.get_color_frame()\n",
    "        depth_frame = frames.get_depth_frame()\n",
    "\n",
    "        # Cleanup:\n",
    "        pipeline.stop()\n",
    "        print(\"Frames Captured\")\n",
    "\n",
    "        color = np.asanyarray(color_frame.get_data())\n",
    "        # Visualization\n",
    "        #plt.rcParams[\"axes.grid\"] = False\n",
    "        #plt.rcParams['figure.figsize'] = [12, 6]\n",
    "        #plt.imshow(color)\n",
    "\n",
    "        colorizer = rs.colorizer()\n",
    "        colorized_depth = np.asanyarray(colorizer.colorize(depth_frame).get_data())\n",
    "        # Visualization\n",
    "        #plt.imshow(colorized_depth)\n",
    "\n",
    "        # Create alignment primitive with color as its target stream:\n",
    "        align = rs.align(rs.stream.color)\n",
    "        frames = align.process(frames)\n",
    "\n",
    "        # Update color and depth frames:\n",
    "        aligned_depth_frame = frames.get_depth_frame()\n",
    "        #print(aligned_depth_frame)\n",
    "        colorized_depth = np.asanyarray(colorizer.colorize(aligned_depth_frame).get_data())\n",
    "\n",
    "        # Show the two frames together:\n",
    "        images = np.hstack((color, colorized_depth))\n",
    "        plt.imshow(images)\n",
    "\n",
    "        #filecolor= \"JPEGImages/\"+img_index+\".jpg\"\n",
    "        #filedepth= \"depth/\"+img_index+\".png\"\n",
    "        filecolor= \"JPEGImages/111.jpg\"\n",
    "        filedepth= \"depth/111.png\"\n",
    "        cv2.imwrite(filecolor, color)\n",
    "        cv2.imwrite(filedepth, colorized_depth)\n",
    "        \n",
    "        \n",
    "        # instance segmentation\n",
    "        image = scipy.misc.imread(\"JPEGImages/111.jpg\")\n",
    "        \n",
    "        # Run detection\n",
    "        results = model.detect([image], verbose=1)\n",
    "        \n",
    "        # Visualize results\n",
    "        r = results[0]\n",
    "        \n",
    "        \n",
    "        pose, depth_copy = generate_stone_pose(aligned_depth_frame, colorized_depth, r)\n",
    "        #plt.figure()\n",
    "        #plt.rcParams['figure.figsize'] = [24, 12]\n",
    "        #plt.imshow(depth_copy)\n",
    "        \n",
    "        visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], \n",
    "                                    class_names, r['scores'])\n",
    "        \n",
    "        stone_pose_msg = stone_pose()\n",
    "        stone_pose_msg.x = pose['x']\n",
    "        stone_pose_msg.y = pose['y']\n",
    "        stone_pose_msg.yaw = pose['yaw']\n",
    "        stone_pose_msg.pitch = pose['pitch']\n",
    "\n",
    "        pose_pub.publish(stone_pose_msg)\n",
    "        print('stone_pose_msg is ', stone_pose_msg)\n",
    "        \n",
    "        is_detect = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
